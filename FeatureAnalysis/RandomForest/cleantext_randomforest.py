# -*- coding: utf-8 -*-
"""CleanText_RandomForest.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mjZLxBgAA86Eufco3oQjPnSR0y0T8Uy2

### General idea
1. Clean dataset => dfClean
2. Vectorize words => to probability density
3. Perform logistic regression on vectorized words 
    of scales of reviews 0 (0,.1),1 (.2,.3) ,2 (.4,.5),3 (.6,.7) ,4 (.8,.9, 1) reviews
"""

# Read data set and stop words
import pandas as pd
import re 
import nltk
import numpy as np
import time
from sklearn.metrics import roc_curve, auc
nltk.download('stopwords')
df = pd.read_csv (r'/content/AppReview.csv')
len(df.index)

# Corpus of stop words
from nltk.corpus import stopwords

# processes a review and returns a list of words
def review_to_words(review, string = True, remove_stopwords=True):
    # Remove HTML
    #review_text = BeautifulSoup(review).get_text()
    review_text=review
    # Remove non-letters
    review_text = re.sub("[^a-zA-Z]"," ", review_text)
    # Convert words to lower case and split them
    words = review_text.lower().split()
    # Optionally remove stop words (false by default)
    if remove_stopwords:
        stops = set(stopwords.words("english"))
        words = [w for w in words if not w in stops]
    if string:
        return " ".join(words)
    else:
        return words

#Clean up text
#Remove non-ascii text
#Remove all rows missing reviewerName
def fixString(x):
    return x.encode('ascii',errors='ignore')

# df[["reviewText"]]=df[["reviewText"]].apply(lambda x: str(x["reviewText"]).encode('ascii',errors='ignore').decode(), axis=1)
# df[["reviewerName"]]=df[["reviewerName"]].apply(lambda x: str(x["reviewerName"]).encode('ascii',errors='ignore').decode(), axis=1)
df[["reviewText"]]=df[["reviewText"]].apply(lambda x: review_to_words(x["reviewText"]), axis=1)
df[["reviewerName"]]=df[["reviewerName"]].apply(lambda x: str(x["reviewerName"]).encode('ascii',errors='ignore').decode(), axis=1)

dfCleaned=df[df['reviewText'].str.strip().astype(bool)]
dfCleaned=dfCleaned[df['reviewerName'].str.strip().astype(bool)]

#
dfCleaned

dfCleaned[['reviewText']]

"""## Vectorize words
### This is based on 
https://towardsdatascience.com/sentiment-analysis-a-how-to-guide-with-movie-reviews-9ae335e6bcb2
With actual logistic regression:
https://towardsdatascience.com/sentiment-classification-with-logistic-regression-analyzing-yelp-reviews-3981678c3b44
"""

from sklearn.model_selection import train_test_split

dfReviews = dfCleaned['reviewText']
dfTrain, dfTest = train_test_split(dfReviews, test_size=.1)

# import statements
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import TfidfTransformer

# Initialize a bag of words
#vectorizer = CountVectorizer(analyzer = "word", tokenizer = None, preprocessor = None, stop_words = None, max_features = 5000) 
vectorizer = CountVectorizer(analyzer = "word", tokenizer = None, preprocessor = None, stop_words = None, max_features = 1000) 

# Fit transform the data 
train_feat = vectorizer.fit_transform(dfTrain).toarray()
test_feat = vectorizer.transform(dfTest).toarray()


# TFIDF train set
tfidf_transformer = TfidfTransformer().fit(train_feat)
train_tfidf = tfidf_transformer.transform(train_feat)
 
# apply tfidf to test set
test_tfidf = tfidf_transformer.transform(test_feat)

#train data
trainYdata = pd.merge(dfTrain.to_frame(), dfCleaned[['reviewerRating']], left_index=True, right_index=True)
#determined feature names
featureNamesList=vectorizer.get_feature_names()

trainYdata.head()

# look at data for training
type(dfTrain)
stopcounter = 0
for index, value in dfTrain.items():
    print(f"Index : {index}, Value : {value}")
    stopcounter=stopcounter+1
    if stopcounter > 10:
        break

# Dump some vectorized words and probablities
import scipy.sparse

featureNamesList=vectorizer.get_feature_names()
type(dfTrain)

cx = scipy.sparse.coo_matrix(train_feat)
cx2 = scipy.sparse.coo_matrix(train_tfidf)


print(dfTrain.iloc[0])
for i,j,v in zip(cx.row, cx.col, cx.data):
    if i!=0:
        break
    print("(%d, %d), %s = %s" % (i,j,featureNamesList[j], v))
    
print("\r\n\r\nThe weighted results\r\n")
for i,j,v in zip(cx2.row, cx2.col, cx2.data):
    if i!=0:
        break
    print("(%d, %d), %s = %s" % (i,j,featureNamesList[j], v))

# look at some more data
import scipy.sparse

featureNamesList=vectorizer.get_feature_names()
type(test_tfidf)

cx = scipy.sparse.coo_matrix(test_tfidf)

print(dfTest.iloc[0])
for i,j,v in zip(cx.row, cx.col, cx.data):
    if i!=0:
        break
    print("(%d, %d), %s = %s" % (i,j,featureNamesList[j], v))

"""### Modelling part
1. Leverage the raw vector count and the tf-idf weighted version
"""

# # Just looking at some data
# train_tfidf
# dfTrain
# print(df.iloc[105146])
# dfTrain
# df
# train_tfidf.todense()
# dfTrain.to_frame()

# Get trained Y data and test Y data
trainYdata = pd.merge(dfTrain.to_frame(), dfCleaned[['reviewerRating']], left_index=True, right_index=True)
testYdata = pd.merge(dfTest.to_frame(), dfCleaned[['reviewerRating']], left_index=True, right_index=True)

# Get trained X data and test X data
trainXdata = train_tfidf.todense()
testXdata = test_tfidf.todense()

from sklearn.datasets import load_iris
# from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier

# Train data - map the y to ints of scales of reviews 0,1,2,3,4 reviews
y = trainYdata[['reviewerRating']]
y_int = trainYdata['reviewerRating'].apply(lambda x: 0 if x<.2 else (1 if x<.4 else (2 if x<.6 else (3 if x<.8 else 4))))

X = trainXdata

# Train data - map the y to ints of scales of reviews 0,1,2,3,4 reviews
y = trainYdata[['reviewerRating']]
y_int = trainYdata['reviewerRating'].apply(lambda x: 0 if x<.2 else (1 if x<.4 else (2 if x<.6 else (3 if x<.8 else 4))))

X = trainXdata
j=25

i_list = []
time_list = []
train_pred_list = []
train_prob_list = []
train_score_list = []
test_pred_list = []
test_prob_list = []
test_score_list = []


# new_dataFrame = pd.DataFrame(data, columns = ['Estimators', 'Time', 'Train Pred', 'Train Prob', 'Train Score', 'Test Pred', 'Test Prob', 'Test Score'])

for i in range(100, 1001, 25):
  start_time = time.time()
  clf = RandomForestClassifier(n_estimators=i,max_depth=j,n_jobs=-1).fit(X, y_int)
  end_time = time.time()
  time_diff = end_time - start_time
  # Check trained accuracy
  train_pred = clf.predict(X)
  train_prob = clf.predict_proba(X)
  train_score = clf.score(X, y_int)
  #Check tested accuracy
  y_int_test = testYdata['reviewerRating'].apply(lambda x: 0 if x<.2 else (1 if x<.4 else (2 if x<.6 else (3 if x<.8 else 4))))
  test_pred = clf.predict(testXdata)
  test_prob = clf.predict_proba(testXdata)
  test_score = clf.score(testXdata, y_int_test)
  i_list.append(i)
  time_list.append(time_diff)
  train_pred_list.append(train_pred)
  train_prob_list.append(train_prob)
  train_score_list.append(train_score)
  test_pred_list.append(test_pred)
  test_prob_list.append(test_prob)
  test_score_list.append(test_score)
  j+=25

new_dict = {'Estimators': i_list, 'Time': time_list, 'Train Pred': train_pred_list, 'Train Prob':  train_prob_list, 'Train Score': train_score_list, 'Test Pred': test_pred_list, 'Test Prob': test_prob_list, 'Test Score': test_score_list}

new_dataFrame = pd.DataFrame(new_dict)

new_dataFrame.to_csv('/content/summ1000.csv')

# np.savetxt("/content/time.csv", time_list, delimiter =", ", fmt ='% s') 

# # Check trained accuracy
# clf.predict(X[:2, :])
# clf.predict_proba(X[:2, :])
# clf.score(X, y_int)
# #Check tested accuracy
# clf.predict(testXdata)
# clf.predict_proba(testXdata)
# clf.score(testXdata, y_int_test)

import matplotlib
import matplotlib.pyplot as plt

plt.xlabel("Estimators")
plt.ylabel("Time")
plt.title("A test graph")
plt.plot(i_list, time_list, 'rx')
# for j in range(len(new_dataFrame)):
#     plt.plot(i_list[j],time_list[j])
plt.legend()
plt.show()
plt.savefig('/content/TM_E1000.png')

plt.xlabel("Estimators")
plt.ylabel("Train Accuracy")
plt.title("A test graph")
plt.plot(i_list, train_score_list, 'gx')
# for j in range(len(new_dataFrame)):
#     plt.plot(i_list[j],train_score_list[j])
plt.legend()
plt.show()
plt.savefig('/content/TRN_A1000.png')

plt.xlabel("Estimators")
plt.ylabel("Test Accuracy")
plt.title("A test graph")
plt.plot(i_list, test_score_list, 'bx')
# for j in range(len(new_dataFrame)):
#     plt.plot(i_list[j],test_score_list[j])
plt.legend()
plt.show()
plt.savefig('/content/TST_A1000.png')

